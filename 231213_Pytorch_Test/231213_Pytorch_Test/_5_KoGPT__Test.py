# -*- coding: cp949 -*-
"""KoGPT2를 이용한 한국어 문장 생성.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OIpVcmFJKexu95aavv4PQWZl7oIO13YW

참고 자료 : https://huggingface.co/blog/how-to-generate
"""



from asyncio.windows_events import NULL
import tensorflow as tf
import torchvision
from transformers import AutoTokenizer
from transformers import TFGPT2LMHeadModel
import numpy as np
import random

import pandas as pd
import tqdm
import urllib.request


def TextGenTest ():
    tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')

    model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)
    
    print("1----------------------------------------")
    sent = '근육이 커지기 위해서는'

    input_ids = tokenizer.encode(sent)
    input_ids = tf.convert_to_tensor([input_ids])
    print(input_ids)

    output = model.generate(input_ids,
                            max_length=128,
                            repetition_penalty=2.0,
                            use_cache=True)
    output_ids = output.numpy().tolist()[0]
    print(output_ids)

    tokenizer.decode(output_ids)

    """# 2. Numpy로 Top 5 뽑기"""
    
    print("2----------------------------------------")

    output = model(input_ids)

    output.logits

    output.logits.shape

    top5 = tf.math.top_k(output.logits[0, -1], k=5)

    tokenizer.convert_ids_to_tokens(top5.indices.numpy())
    
    print("3----------------------------------------")
    """# 3. Numpy Top 5로 문장 생성하기"""

    sent = '근육이 커지기 위해서는'
    input_ids = tokenizer.encode(sent)
    
    
    print("4----------------------------------------")
    while len(input_ids) < 50:
        output = model(np.array([input_ids]))
        top5 = tf.math.top_k(output.logits[0, -1], k=5)
        token_id = random.choice(top5.indices.numpy())
        input_ids.append(token_id)

    print("5----------------------------------------")
    strOut = tokenizer.decode(input_ids)
    
    print(strOut)
    

def TextGenLoop(): 
    tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')

    model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)
    
    """# 3. Numpy Top 5로 문장 생성하기"""

    while(True) :
        sent = input("input : ")
        input_ids = tokenizer.encode(sent)
    
    
        while len(input_ids) < 50:
            output = model(np.array([input_ids]))
            top5 = tf.math.top_k(output.logits[0, -1], k=5)
            token_id = random.choice(top5.indices.numpy())
            input_ids.append(token_id)

        strOut = tokenizer.decode(input_ids)
    
        print(strOut)
        


def ChatbotTest () :

    tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', pad_token='<pad>')
    model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)

    print(tokenizer.bos_token_id)
    print(tokenizer.eos_token_id)
    print(tokenizer.pad_token_id)
    print('-' * 10)
    print(tokenizer.decode(1))
    print(tokenizer.decode(2))
    print(tokenizer.decode(3))
    print(tokenizer.decode(4))


    urllib.request.urlretrieve("https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv", filename="ChatBotData.csv")
    train_data = pd.read_csv('ChatBotData.csv')

    len(train_data)

    batch_size = 32

    def get_chat_data():
      for question, answer in zip(train_data.Q.to_list(), train_data.A.to_list()):
        bos_token = [tokenizer.bos_token_id]
        eos_token = [tokenizer.eos_token_id]
        sent = tokenizer.encode('<usr>' + question + '<sys>' + answer) 
        yield bos_token + sent + eos_token

    dataset = tf.data.Dataset.from_generator(get_chat_data, output_types=tf.int32)

    dataset = dataset.padded_batch(batch_size=batch_size, padded_shapes=(None,), padding_values=tokenizer.pad_token_id)

    for batch in dataset:
        print(batch)
        break

    tokenizer.decode(batch[0])

    print(batch[0])

    print(tokenizer.encode('</s><usr> 12시 땡!<sys> 하루가 또 가네요.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'))

    adam = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)

    steps = len(train_data) // batch_size + 1
    print(steps)

    EPOCHS = 3

    for epoch in range(EPOCHS):
      epoch_loss = 0
      count = 0
      for batch in tqdm.tqdm_notebook(dataset, total=steps):
          count += 1
          with tf.GradientTape() as tape:
              result = model(batch, labels=batch)
              loss = result[0]
              batch_loss = tf.reduce_mean(loss)
          
          grads = tape.gradient(batch_loss, model.trainable_variables)
          adam.apply_gradients(zip(grads, model.trainable_variables))
          epoch_loss += batch_loss / steps
          print('[Count: {:>4}/{:>4}] cost = {:>.9}'.format(count + 1,steps, epoch_loss))

      print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, epoch_loss))

    text = '오늘도 좋은 하루!'

    sent = '<usr>' + text + '<sys>'

    input_ids = [tokenizer.bos_token_id] + tokenizer.encode(sent)
    input_ids = tf.convert_to_tensor([input_ids])

    output = model.generate(input_ids, max_length=50, early_stopping=True, eos_token_id=tokenizer.eos_token_id)

    decoded_sentence = tokenizer.decode(output[0].numpy().tolist())

    decoded_sentence.split('<sys> ')[1].replace('</s>', '')

    output = model.generate(input_ids, max_length=50, do_sample=True, top_k=10)
    tokenizer.decode(output[0].numpy().tolist())

    def return_answer_by_chatbot(user_text):
      sent = '<usr>' + user_text + '<sys>'
      input_ids = [tokenizer.bos_token_id] + tokenizer.encode(sent)
      input_ids = tf.convert_to_tensor([input_ids])
      output = model.generate(input_ids, max_length=50, do_sample=True, top_k=20)
      sentence = tokenizer.decode(output[0].numpy().tolist())
      chatbot_response = sentence.split('<sys> ')[1].replace('</s>', '')
      return chatbot_response

    return_answer_by_chatbot('안녕! 반가워~')

    return_answer_by_chatbot('너는 누구야?')

    return_answer_by_chatbot('사랑해')

    return_answer_by_chatbot('나랑 영화보자')

    return_answer_by_chatbot('너무 심심한데 나랑 놀자')

    return_answer_by_chatbot('영화 해리포터 재밌어?')

    return_answer_by_chatbot('너 딥 러닝 잘해?')

    return_answer_by_chatbot('너 취했어?')

    return_answer_by_chatbot('커피 한 잔 할까?')
    
    
def ChatBotLoop() :
    tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', pad_token='<pad>')
    model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)
    
    urllib.request.urlretrieve("https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv", filename="ChatBotData.csv")
    train_data = pd.read_csv('ChatBotData.csv')
    model.save("my_model.keras");

    batch_size = 32
    def get_chat_data():
      for question, answer in zip(train_data.Q.to_list(), train_data.A.to_list()):
        bos_token = [tokenizer.bos_token_id]
        eos_token = [tokenizer.eos_token_id]
        sent = tokenizer.encode('<usr>' + question + '<sys>' + answer) 
        yield bos_token + sent + eos_token

    dataset = tf.data.Dataset.from_generator(get_chat_data, output_types=tf.int32)
    dataset = dataset.padded_batch(batch_size=batch_size, padded_shapes=(None,), padding_values=tokenizer.pad_token_id)

    adam = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)

    steps = len(train_data) // batch_size + 1

    EPOCHS = 3

    for epoch in range(EPOCHS):
        epoch_loss = 0
        count = 0
        for batch in tqdm.tqdm_notebook(dataset, total=steps):
            count += 1
            with tf.GradientTape() as tape:
                result = model(batch, labels=batch)
                loss = result[0]
                batch_loss = tf.reduce_mean(loss)
          
            grads = tape.gradient(batch_loss, model.trainable_variables)
            adam.apply_gradients(zip(grads, model.trainable_variables))
            epoch_loss += batch_loss / steps
            print('[Count: {:>4}/{:>4}] cost = {:>.9}'.format(count + 1,steps, epoch_loss))
      
        print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, epoch_loss))
        
        # 저장
        if(model == NULL) :
           print("save Failure!")
           continue
        model.save("my_model.keras");
        print("save Succeed!")


    def return_answer_by_chatbot(user_text):
      sent = '<usr>' + user_text + '<sys>'
      input_ids = [tokenizer.bos_token_id] + tokenizer.encode(sent)
      input_ids = tf.convert_to_tensor([input_ids])
      output = model.generate(input_ids, max_length=50, do_sample=True, top_k=20)
      sentence = tokenizer.decode(output[0].numpy().tolist())
      chatbot_response_list = sentence.split('<sys>')
      chatbot_response = chatbot_response_list[1].replace('</s>', '')
      return chatbot_response
    
    while(True) :
        question  = input("질문 : ")
        answer = return_answer_by_chatbot(question)
        print("답장 : " + answer)
def ChatBotQandA() :
    tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', pad_token='<pad>')
    model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)

    model.load_weights("my_model.keras");

    def return_answer_by_chatbot(user_text):
      sent = '<usr>' + user_text + '<sys>'
      input_ids = [tokenizer.bos_token_id] + tokenizer.encode(sent)
      input_ids = tf.convert_to_tensor([input_ids])
      output = model.generate(input_ids, max_length=50, do_sample=True, top_k=20)
      sentence = tokenizer.decode(output[0].numpy().tolist())
      chatbot_response_list = sentence.split('<sys>')
      chatbot_response = chatbot_response_list[1].replace('</s>', '')
      return chatbot_response
    
    while(True) :
        question  = input("질문 : ")
        answer = return_answer_by_chatbot(question)
        print("답장 : " + answer)

  
  


    
# from tkinter import W
# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# from torch.nn.modules import linear
# import torchmetrics
# from torch.utils.data import Dataset, DataLoader
# from torch import optim

# import numpy as np

# import torchvision.transforms as transforms
# from torchvision import datasets
# import matplotlib.pyplot as plt

# import gpt_2_simple as gpt2


def Init():
   sess = gpt2.start_tf_sess()
   gpt2.finetune(sess, 'shakespeare.txt', steps=1000)   # steps is max number of training steps
   print(gpt2.generate(sess))
